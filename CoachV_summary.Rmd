---
title:  "A summary of mid-distance race analyses"
author:  "Mitchell Greenhalgh"
date:  "2023-12-23"
output:  
  html_document:
    toc: true
    theme: lumen
---

```{=html}
<style type="text/css">
h1.title {
  font-size: 40px;
  text-align: center;
}
.author, .date {
  font-size: 25px;
  text-align: center;
}
div#TOC {
    padding-left: 25%;
    padding-right: 25%;
}
.table td:first-child {
    vertical-align: middle;
}
hr {
    border: 1px solid black; 
    background-color:black;
}
</style>
```

Hi V,

I've alluded to this project to you a couple of times over the last couple years, but I've never gotten around to putting this 'report' together until now. The origins of this project come from my love for mid-distance running, especially the 400m and 800m. There's something about racing on the double-edged blade of the anaerobic-aerobic threshold that's inspiring, captivating, and a lot of fun. At some point, every person who coaches or runs these events asks themselves, "What's the best way to run these?" The answer is obviously one word: fast. But in my head, that question evolved into, "What's the most efficient way to run them?" And after discovering statistics after not taking your class in high school, which I still regret to a small degree, I started thinking about how there must be a certain way to run these events such that on average you'll be able to run faster.

This diving board took me down several avenues. I used several statistical methods in an attempt to see which could really explain efficiency, and vicariously success, in mid-distance. Clustering, dimension reduction, random forests, multiple regression, dozens of race metrics, and other approaches didn't elucidate a story from the data. Then, as I served as a statistical consultant on a master's project, I learned two new methods that illuminated something important. I don't take the things I've learned as the full truth yet: I still have relatively little data and it's hard to get truly independent race data. The most available data I have found yet come from championship racing and prelims. These data were very cloudy, however, and tainted by intentional race tactics that weren't necessarily motivated by trying to run as fast as possible. But, I think with the rest of the data that I do have, a very interesting story is told.

<hr>

## Methods

#### 800m Data

Some of the data I used were collected from your track notes between 2010ish and 2016ish. Another portion consists of the athletes I coach right now. The last portion of data, and the most precise data, comes from NBNI (New Balance Nationals Indoor) high school championships. Although they were championship data, in high school the championships are usually more about running fast than NCAA or international stuff. They had about the same proportion of negative splits as the rest of the data that I collected, so I think they're safe for analysis (negative splits were a much larger proportion of NCAA prelims and finals). I'm thinking about trying to get more NBNI competitions, but if I do, I want to automate it. You can say a little bit about how indoor races are different than outdoor, and I know some mixed-effect modeling that can account for those differences, but I think they're relatively inconsequential. For all the data sources, I collected 400m splits, and some had 200m splits, but I didn't end up using to 200m splits for the 800m analysis.

#### 400m Data

The two data sources for the 400m data come from my coaching stuff and NBNI 2023 again. NBNI was really the only place to publish 200m splits for the 400s (makes sense for 200m tracks).

#### Race Metrics

I tried a bunch of metrics from races to measure efficiency, and what I ended up going with was the ratio of the second half of the race to the first half (generally thought about as a ratio of laps):

$$\frac{\text{lap}_2}{\text{lap}_1}$$

This meant that $x = 1.0$ is an even split, $x > 1.0$ is a slower second lap (which is typical), and $x < 1.0$ is a negative split.

#### Smoothing curves for basic trends

When I first put the dataset together, I used a LOESS smoothing curve to look at the data for a general trend. I was actually taken aback by how strong of a pattern there was in the data. I don't know what I was expecting, but the pattern was very defined, especially for the 400m. I think with more data the 800m will more closely resemble the 400m.

#### Changepoint Analysis and Piecewise Regression

One of the methods I referred to/learned is called changepoint or breakpoint analysis. Essentially, they're algorithms that detect a change in average, variance, or slope in a sequence of data points. This is used in tandem with piecewise regression, where post hoc models are fitted to segments in the sequence, according to detected changepoints. So, in this case study, I arranged all the data according to their laps ratio, then fit linear models to each side of the changepoint. I don't really think there's a ton of merit to the linear models by themselves, but it does serve as a solid conceptualization of the pattern that's going on. I think a natural spline, or maybe a quadratic model, would serve as a better regression method, but the point is that there's a changepoint that occurs in the lap ratios, suggesting an optimal speed differential between your first and second lap (or first and second 200m split).

<hr>

## Results

```{r setup, include = FALSE}
# Remove code, warnings, messages, and errors when knitting, put all figures in center
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE, 
                      fig.align = 'center')

library(tidyverse)  # Pipelines, etc.
library(ggfortify)  # Linear Model plotting in [ggplot]
library(kableExtra)  # HTML table styling so it doesn't look like the R console
library(segmented)
library(RSQLite)

rm(list = ls())


# Function to simplify adding kable tables. Makes titles bold.
addKable <- function(data, caption){
  data %>% 
    kbl(caption = paste0("<b>", caption),
        align = "l") %>%
    kable_styling(bootstrap_options = c("striped"),
                  full_width = FALSE,
                  position = "left")
}

# Make all the NA values in kable tables empty
options(knitr.kable.NA = "")

# Plot themes
theme_sq <- theme_bw() + theme(aspect.ratio = 1)
theme_wide <-  theme_bw() + theme(aspect.ratio = 1/1.618)
```

```{r import-data}
driver = dbDriver('SQLite')
db_path = 'C:\\Users\\mitch\\Documents\\GitHub\\mid_distance_db\\middistanceDB_master.db'

db = dbConnect(drv = driver,
               dbname = db_path)

d800 = dbReadTable(db, 'splits_800m')
d400 = dbReadTable(db, 'splits_400m')
```

```{r 800m-analysis}
plot_800_smooth <- ggplot(d800, aes(x = split_ratio, y = total_time_sec)) +
  geom_point() + 
  geom_smooth() +
  theme_bw() + 
  theme(aspect.ratio = 1)

model_800 <- lm(total_time_sec ~ split_ratio, data = d800)
bpoint_800 <- segmented::segmented(model_800, npsi = 1)$psi[2]  # 1.040


plot_800_linear <- ggplot(d800, aes(x = split_ratio, y = total_time_sec)) + 
  geom_point() + 
  geom_smooth(data = d800 %>% filter(split_ratio <= bpoint_800),
    method = 'lm', se = FALSE, size = 1.5) +
  geom_smooth(data = d800 %>% filter(split_ratio > bpoint_800),
    method = 'lm', se = FALSE, size = 1.5) +
  geom_vline(xintercept = c(bpoint_800), color = 'darkred', size = 1.5) +
  theme_bw() + 
  theme(aspect.ratio = 1) + 
  scale_y_continuous(limits = c(110, 190),
                     breaks = seq(110, 190, 10))
```

### 800m

#### Smoothing Curves

This is a the first look I took at the data, with the smoothing curve.

```{r loess-800m}
plot_800_smooth + 
  labs(x = 'Lap Ratio',
       y = 'Time (seconds)',
       title = '800m Race Efficiency') + 
  theme(plot.title = element_text(hjust = 0.5))
```

The changepoint for this was $1.049$, meaning that you run the second lap 4.9% slower than the first. This is what the linear models would look like at that changepoint.

```{r changepoint-800m}
plot_800_linear + 
  labs(title = '800m Linear Models',
       x = 'Lap Ratio',
       y = 'Time (seconds)') + 
  theme(plot.title = element_text(hjust = 0.5))
```

Clearly, the goal is to run as close to an even split as possible, but even in David Rudisha's world record, he didn't run even splits. So, there is an advantage to run the first lap faster.

#### Quadratic Model

```{r quadratic-800}
data_combined_quadr <- d800 %>% mutate(split_ratio_2 = split_ratio**2)
quadr_800_model <- lm(total_time_sec ~ split_ratio + split_ratio_2 + I(split_ratio**3), data = data_combined_quadr)

ratio_vals <- seq(min(data_combined_quadr$split_ratio), 
                  max(data_combined_quadr$split_ratio), 
                  0.0001)
predicted_vals <- predict(quadr_800_model, 
                          list(split_ratio = ratio_vals, split_ratio_2 = ratio_vals**2))
predicted_ci <- predict(quadr_800_model, 
                        list(split_ratio = ratio_vals, split_ratio_2 = ratio_vals**2), 
                        interval = 'confidence')


print(paste('Approximate Global Minimum:', ratio_vals[which.min(predicted_vals)] %>% round(4)))  # 1.0537



ggplot() + 
  geom_point(data = d800,
             aes(x = split_ratio, y = total_time_sec),
             alpha = 0.75,
             color = 'darkgrey') + 
  geom_line(data = tibble(ratio_vals, predicted_vals), 
            aes(x = ratio_vals, y = predicted_vals),
            size = 1.5,
            color = 'darkred') + 
  geom_line(data = predicted_ci,
            aes(x = ratio_vals, y = upr),
            size = 1.5,
            color = 'darkred',
            lty = 2) + 
  geom_line(data = predicted_ci,
            aes(x = ratio_vals, y = lwr),
            size = 1.5,
            color = 'darkred',
            lty = 2) +
  theme_sq
```

Another way of looking at the pattern is through quadratic regression. The approximate apex of the curve lies at $1.054$, which is relatively close to the detected breakpoint. This model breaks a few assumptions of linearity, but visualizing a quadratic model given the LOESS was a helpful exercise.

### 400m

#### Smoothing Curves

The 400m data had an even stronger trend than the 800m. Here's what the data looked like.

```{r data-wrangling-400m}
data_400 <- read_csv('2023_nbni_400m.csv')
# glimpse(data)

data_400 <- data_400 %>% mutate(split_ratio = second_200 / first_200)
```

```{r loess-and-changepoint-400m}
ggplot(data_400, aes(x = split_ratio, y = total_time_sec)) + 
  geom_point() + 
  geom_smooth() +
  labs(x = 'Split Ratio',
       y = 'Time (seconds)',
       title = '400m Race Efficiency') + 
  theme_sq + 
  theme(plot.title = element_text(hjust = 0.5))

model_400 <- lm(total_time_sec ~ split_ratio, data = data_400)
bpoint_400 <- segmented::segmented(model_400, npsi = 1)$psi[2]  # 1.123

ggplot(data_400, aes(x = split_ratio, y = total_time_sec)) + 
  geom_point() + 
  geom_smooth(data = data_400 %>% filter(split_ratio <= bpoint_400),
    method = 'lm', se = FALSE, size = 1.5) +
  geom_smooth(data = data_400 %>% filter(split_ratio > bpoint_400),
    method = 'lm', se = FALSE, size = 1.5) +
  geom_vline(xintercept = c(bpoint_400), color = 'darkred', size = 1.5) +
  theme_bw() + 
  theme(aspect.ratio = 1) + 
  scale_y_continuous(limits = c(45, 75),
                     breaks = seq(45, 75, 5)) + 
  labs(x = 'Split Ratio',
       y = 'Time (seconds)',
       title = '400m Linear Models') + 
  theme(plot.title = element_text(hjust = 0.5))
```

The changepoint that was detected in the 400m data occurred at $1.123$. It's apparent that in the 400m, you need to run the first half of the race proportionally faster than you would in the 800m.

#### Quadratic Model

```{r quadratic-400}
data_400_quadr <- data_400 %>% mutate(split_ratio_2 = split_ratio**2)
quadr_400_model <- lm(total_time_sec ~ split_ratio + split_ratio_2, data = data_400_quadr)

ratio_vals_400 <- seq(min(data_400_quadr$split_ratio), 
                  max(data_400_quadr$split_ratio), 
                  0.0001)
predicted_vals_400 <- predict(quadr_400_model, 
                          list(split_ratio = ratio_vals_400, split_ratio_2 = ratio_vals_400**2))
predicted_ci_400 <- predict(quadr_400_model, 
                        list(split_ratio = ratio_vals_400, split_ratio_2 = ratio_vals_400**2), 
                        interval = 'confidence')

print(paste('Approximate Global Minimum:', ratio_vals_400[which.min(predicted_vals_400)] %>% round(4)))    # 1.1492

ggplot() + 
  geom_point(data = data_400,
             aes(x = split_ratio, y = total_time_sec),
             alpha = 0.75,
             color = 'darkgrey') + 
  geom_line(data = tibble(ratio_vals_400, predicted_vals_400), 
            aes(x = ratio_vals_400, y = predicted_vals_400),
            size = 1.5,
            color = 'darkred') + 
  geom_line(data = predicted_ci_400,
            aes(x = ratio_vals_400, y = upr),
            size = 1.5,
            color = 'darkred',
            lty = 2) + 
  geom_line(data = predicted_ci_400,
            aes(x = ratio_vals_400, y = lwr),
            size = 1.5,
            color = 'darkred',
            lty = 2) +
  theme_sq
```

Again, this quadratic model violates some assumptions, but it's interesting to note how the approximate global minimum is less than the 400m's estimated breakpoint, while the 800m's minimum was greater than its breakpoint.

<hr>

## Discussion and Conclusion

Like I said earlier, I don't think this is a final draft of whatever these data might be telling us. For example, the changepoints that were detected are the ratios used among the fastest times. That doesn't mean that a 2:10 800m runner needs to run that ratio, they are likely better off with having a slightly larger difference between laps. But it's interesting to think about optimizing your running economy in these races. I think a definite take away is that in order to run faster in the 800m, you need to make your race more consistent than the 400m, but the absolute ratio that produces that optimal speed will change from runner to runner. It could serve as a starting point and/or goal to replicate that lap ratio in your races, but at the end of the day it's how you balance your speed versus your endurance/strength in your training.

Let me know if you have any questions or feedback on all of this, I'm going to keep gathering data as time goes on. One day when I run my own coaching program, I'm gonna get all the data I could ever need hahaha. I love you, V, thank you for helping me discover a lifelong passion all those years ago.
